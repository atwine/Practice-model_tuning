{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning Practice\n",
    "\n",
    "By `Atwine Mugume Twinamatsiko`\n",
    "\n",
    "I have been looking at some kernels on Model tuning using hyperopt and other implimentations so I wat to try it out myself and see the power of what it can do\n",
    "\n",
    "This is a great [Article](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc) to read on lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's read in the data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rapid prototyping, let's take a few rows of the training data\n",
    "train_sample = train.sample(n = 20000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BatchId_0</th>\n",
       "      <th>BatchId_1</th>\n",
       "      <th>BatchId_2</th>\n",
       "      <th>BatchId_3</th>\n",
       "      <th>BatchId_4</th>\n",
       "      <th>BatchId_5</th>\n",
       "      <th>BatchId_6</th>\n",
       "      <th>BatchId_7</th>\n",
       "      <th>BatchId_8</th>\n",
       "      <th>...</th>\n",
       "      <th>TransactionStartTimeDayofweek</th>\n",
       "      <th>TransactionStartTimeDayofyear</th>\n",
       "      <th>TransactionStartTimeIs_month_end</th>\n",
       "      <th>TransactionStartTimeIs_month_start</th>\n",
       "      <th>TransactionStartTimeIs_quarter_end</th>\n",
       "      <th>TransactionStartTimeIs_quarter_start</th>\n",
       "      <th>TransactionStartTimeIs_year_end</th>\n",
       "      <th>TransactionStartTimeIs_year_start</th>\n",
       "      <th>TransactionStartTimeElapsed</th>\n",
       "      <th>FraudResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>319</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1542248329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>319</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1542248348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>319</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1542249861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>319</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1542252775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  BatchId_0  BatchId_1  BatchId_2  BatchId_3  BatchId_4  \\\n",
       "0           0          0          0          1          0          1   \n",
       "1           1          0          0          0          1          1   \n",
       "2           2          0          0          1          1          1   \n",
       "3           3          0          0          0          0          0   \n",
       "\n",
       "   BatchId_5  BatchId_6  BatchId_7  BatchId_8     ...       \\\n",
       "0          1          0          1          1     ...        \n",
       "1          1          1          1          0     ...        \n",
       "2          0          1          0          1     ...        \n",
       "3          0          0          1          1     ...        \n",
       "\n",
       "   TransactionStartTimeDayofweek  TransactionStartTimeDayofyear  \\\n",
       "0                              3                            319   \n",
       "1                              3                            319   \n",
       "2                              3                            319   \n",
       "3                              3                            319   \n",
       "\n",
       "   TransactionStartTimeIs_month_end  TransactionStartTimeIs_month_start  \\\n",
       "0                             False                               False   \n",
       "1                             False                               False   \n",
       "2                             False                               False   \n",
       "3                             False                               False   \n",
       "\n",
       "   TransactionStartTimeIs_quarter_end  TransactionStartTimeIs_quarter_start  \\\n",
       "0                               False                                 False   \n",
       "1                               False                                 False   \n",
       "2                               False                                 False   \n",
       "3                               False                                 False   \n",
       "\n",
       "   TransactionStartTimeIs_year_end  TransactionStartTimeIs_year_start  \\\n",
       "0                            False                              False   \n",
       "1                            False                              False   \n",
       "2                            False                              False   \n",
       "3                            False                              False   \n",
       "\n",
       "   TransactionStartTimeElapsed  FraudResult  \n",
       "0                   1542248329            0  \n",
       "1                   1542248348            0  \n",
       "2                   1542249861            0  \n",
       "3                   1542252775            0  \n",
       "\n",
       "[4 rows x 92 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take out the label of the data\n",
    "label = train_sample['FraudResult']\n",
    "train_sample = train_sample.drop(columns=['FraudResult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95662, 92), (45019, 91))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's split the data and get to it already\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_sample,\n",
    "                                                    label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29934, 91), (29934,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have an imbalance in the data so we are going to do SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=27,ratio='minority')\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "#Let's see the shape\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 14967\n",
      "After OverSampling, counts of label '0': 14967\n"
     ]
    }
   ],
   "source": [
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (29934, 91)\n",
      "Testing features shape:  (5000, 91)\n"
     ]
    }
   ],
   "source": [
    "#here we want to see what the shapes of the data look like.\n",
    "print(\"Training features shape: \", X_train.shape)\n",
    "print(\"Testing features shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Since we are have a small dataset we can't curve out a validation set, in this case we are going to use crossvalidation instead.\n",
    "\n",
    "We are going to also impliment early stopping, because as the model builds more trees the more efficient it becomes also complex and the more the the loss reduces.\n",
    "\n",
    "In the case of the GBM, this means training more decision trees, and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation error has not decreased for 100 rounds. Then, the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to use cross validation with lignGBM we need to create an an LGBM dataset\n",
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = X_train, label = y_train)\n",
    "test_set = lgb.Dataset(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:741: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    }
   ],
   "source": [
    "#in the code below we carry out cross validation with 100 rounds early stopping\n",
    "\n",
    "model = lgb.LGBMClassifier() #initialize the model\n",
    "default_params = model.get_params() #get the default hyperparameters\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators'] #remove the default estimators\n",
    "#please note: num_boost_round is the same as n_estimators\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need the f1_score metric also because that is what the competition is based on\n",
    "def print_f1(model):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.fit(X_train,y_train)\n",
    "    #what ever model we choose we run a predict test on it\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    #print the f1 score\n",
    "    return f1_score(y_test,pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the f1 score of the model above is here.\n",
    "print_f1(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation ROC AUC was: 1.00000 with a standard deviation of 0.00000.\n",
      "The optimal number of boosting rounds (estimators) was 62.\n"
     ]
    }
   ],
   "source": [
    "#lets have a look at the dictionary with the cv_resutls\n",
    "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we use the result above as the baseline in order to beat if we had a lover one, however we are going to continue to implement a working solution for hyperparameter tuning using hyperopt (auto tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model scores 0.99991 ROC AUC on the test set.\n",
      "The baseline model scores 0.82353 F1_score on the test set.\n"
     ]
    }
   ],
   "source": [
    "#let's check how our model is doing on our test data\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "\n",
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "#here I compute the f1 score manually.\n",
    "preds_ = model.predict(X_test)\n",
    "baseline_f1 = f1_score(y_test, preds_)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))\n",
    "print('The baseline model scores {:.5f} F1_score on the test set.'.format(baseline_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning implimentation\n",
    "\n",
    "### Four parts of Hyperparameter tuning\n",
    "\n",
    "It's helpful to think of hyperparameter tuning as having four parts (these four parts also will form the basis of Bayesian Optimization):\n",
    "\n",
    "1. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize\n",
    "2. Domain: the set of hyperparameter values over which we want to search. \n",
    "3. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.\n",
    "4. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function.\n",
    "\n",
    "Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts. \n",
    "\n",
    "#### Objective Function\n",
    "\n",
    "The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the F1 which of course we want to maximize. Later, when we get to Bayesian Optimization, we will have to use a value to minimize, so we can take $1 - \\text{F1}$ as the score. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation F1. This score will then be used to select the best model hyperparameter values. \n",
    "\n",
    "In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain, and in this case this will be the hyperparameters which the algorithm is going to search through in order to get us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a default model to show hyper parameters\n",
    "# We don't need to tune all the parameters, just some of them.\n",
    "model = lgb.LGBMModel()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results History\n",
    "\n",
    "The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. Random and grid search are _uninformed_ methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! \n",
    "\n",
    "A dataframe is a useful data structure to hold the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes for random and grid search\n",
    "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Implementation\n",
    "\n",
    "We are trying to find a way to make this method less computationally expensive because it works in such a way that it fits all values in memory and uses a lot of resources. \n",
    "\n",
    "A good way to avoid this is to unpack the dictionary of the parameter grid using the random function such that at each iteration we shall have a set of values that are evaluated and the results stored and learnt from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    \n",
    "    keys, values = zip(*param_grid.items())#this bundles the dictionary so that items match\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results #this adds the input into the dataframe which will keep results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best validation score was 1.00000\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'gbdt',\n",
      " 'colsample_bytree': 0.6,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 0.004999999999999999,\n",
      " 'min_child_samples': 20,\n",
      " 'n_estimators': 58,\n",
      " 'num_leaves': 20,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'subsample': 0.5,\n",
      " 'subsample_for_bin': 20000}\n"
     ]
    }
   ],
   "source": [
    "grid_results = grid_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params']) #since we sorted these in the function above, the top row has the best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some parameters let us evaluate them and see what scores they return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from grid search scores 0.99994 ROC AUC on the test set.\n",
      "The best model from grid search scores 0.82353 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "grid_search_params = grid_results.loc[0, 'params']\n",
    "\n",
    "# Create, train, test model\n",
    "model = lgb.LGBMClassifier(**grid_search_params, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#check the f1_score of the new model.\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "pred_ = model.predict(X_test)\n",
    "\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(y_test, preds)))\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(f1_score(y_test, preds_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "\n",
    "This  method is better than grid search terms of exploring the sample pace of the variables that we have. It helps us come close to the best parameters through random choice.\n",
    "\n",
    "Let's reimplemet it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    import random #from this function we will be able to choose randomly from the parameters \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best validation score was 1.00000\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'gbdt',\n",
      " 'colsample_bytree': 0.7333333333333333,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 0.26711466497691755,\n",
      " 'min_child_samples': 425,\n",
      " 'n_estimators': 534,\n",
      " 'num_leaves': 80,\n",
      " 'reg_alpha': 0.2857142857142857,\n",
      " 'reg_lambda': 0.8163265306122448,\n",
      " 'subsample': 0.9040404040404041,\n",
      " 'subsample_for_bin': 120000}\n"
     ]
    }
   ],
   "source": [
    "#let's take it for a spin\n",
    "random_results = random_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
    "print('The best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(random_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from random search scores 0.99997 ROC AUC on the test set.\n",
      "The best model from random search scores 0.85714 F1_Score on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "\n",
    "# Create, train, test model\n",
    "model = lgb.LGBMClassifier(boosting_type='dart',colsample_bytree=0.77777777778,is_unbalance=True,learning_rate=0.4404685952236995,min_child_samples=400,\n",
    "                           n_estimators=10000,num_leaves=91,reg_alpha=0.26530612244897955,reg_lambda=0.22448979591836732,subsample=0.83333333333334\n",
    "                           ,subsample_for_bin=160000, random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pred_ = model.predict(X_test)\n",
    "\n",
    "print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(y_test, preds)))\n",
    "print('The best model from random search scores {:.5f} F1_Score on the test set.'.format(f1_score(y_test, pred_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4992    1]\n",
      " [   1    6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "When we run this computation we would like to do it as many times as possible that is for the random search, this is in order to give it enough time to randomly choose various parameters which will probably yield the best values on our dataset.\n",
    "\n",
    "In order to be able to keep a record of the cycles and results we are going to implement code to help us write the results to a file, csv format.\n",
    "\n",
    "### Extremely Important Note about Checking Files\n",
    "\n",
    "When you want to check the csv file, __do not open it in Excel while the search is ongoing__. This will cause a permission error in Python and the search will be terminated. Instead, you can view the end of the file by typing `tail out_file.csv` from Bash where `out_file.csv` is the name of the file being written to. There are also some text editors, such as notepad or Sublime Text, where you can open the results safely while the search is occurring. However, __do not use Excel to open a file that is being written to in Python__. This is a mistake I've made several times so you do not have to! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will help us create the file we are going to use\n",
    "import csv\n",
    "\n",
    "# Create file and open connection\n",
    "out_file = 'random_search_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must slightly modify `random_search` and `grid_search` to write to this file every time. We do this by opening a connection, this time using the `\"a\"` option for append (the first time we used the `\"w\"` option for write) and writing a line with the desired information (which in this case is the cross validation score, the hyperparameters, and the number of the iteration). Then we close the connection until the function is called again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization. \n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(random_params, i)\n",
    "        results.loc[i, :] = eval_results\n",
    "\n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        # Select the hyperparameters\n",
    "        parameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        parameters['subsample'] = 1.0 if parameters['boosting_type'] == 'goss' else parameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(parameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the functions set up, let's run them a couple of times and see which ones bring us the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "/anaconda3/lib/python3.6/site-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done here!\n"
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 10\n",
    "\n",
    "import random\n",
    "\n",
    "# Create file and open connection\n",
    "out_file = 'grid_search_trials_1.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "grid_results = grid_search(param_grid, out_file)\n",
    "\n",
    "\n",
    "# Create file and open connection\n",
    "out_file = 'random_search_trials_1.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write column names\n",
    "headers = ['score', 'hyperparameters', 'iteration']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "random_results = random_search(param_grid, out_file)\n",
    "\n",
    "print('All done here!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Limited Data\n",
    "\n",
    "We can examine 1000 search iterations of the above functions on the reduced dataset. Later, we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times! The 1000 search iterations were not run in a kernel, although they might be able to finish (no guarantees) in the 12 hour time limit. \n",
    "\n",
    "First we can find out which method returned the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = pd.read_csv('random_search_trials_1.csv')\n",
    "grid_results = pd.read_csv('grid_search_trials_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we save the results to a csv, for some reason the dictionaries are saved as strings. Therefore we need to convert them back to dictionaries after reading in the results using the `ast.literal_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert strings to dictionaries\n",
    "grid_results['hyperparameters'] = grid_results['hyperparameters'].map(ast.literal_eval)\n",
    "random_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to parse the results from the hyperparameter searches. This returns a dataframe where each column is a hyperparameter and each row has one search result (so taking the dictionary of hyperparameters and mapping it into a row in a dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results, name):\n",
    "    \"\"\"Evaluate model on test data using hyperparameters in results\n",
    "       Return dataframe of hyperparameters\"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    # Sort with best values on top\n",
    "    results = results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Print out cross validation high score\n",
    "    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, results.loc[0, 'score'], results.loc[0, 'iteration']))\n",
    "    \n",
    "    # Use best hyperparameters to create a model\n",
    "    hyperparameters = results.loc[0, 'hyperparameters']\n",
    "    model = lgb.LGBMClassifier(**hyperparameters)\n",
    "    \n",
    "    # Train and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    #print the fl score\n",
    "    preds_ = model.predict(X_test)\n",
    "    \n",
    "    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(y_test, preds)))\n",
    "    print('F1 from {} on test data = {:.5f}.'.format(name, f1_score(y_test, preds_)))\n",
    "    \n",
    "    # Create dataframe of hyperparameters\n",
    "    hyp_df = pd.DataFrame(columns = list(results.loc[0, 'hyperparameters'].keys()))\n",
    "\n",
    "    # Iterate through each set of hyperparameters that were evaluated\n",
    "    for i, hyp in enumerate(results['hyperparameters']):\n",
    "        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n",
    "                               ignore_index = True)\n",
    "        \n",
    "    # Put the iteration and score in the hyperparameter dataframe\n",
    "    hyp_df['iteration'] = results['iteration']\n",
    "    hyp_df['score'] = results['score']\n",
    "    \n",
    "    return hyp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest cross validation score from grid search was 1.00000 found on iteration 0.\n",
      "ROC AUC from grid search on test data = 0.99969.\n",
      "F1 from grid search on test data = 0.73684.\n"
     ]
    }
   ],
   "source": [
    "grid_hyp = evaluate(grid_results, name = 'grid search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest cross validation score from random search was 1.00000 found on iteration 9.\n",
      "ROC AUC from random search on test data = 0.99997.\n",
      "F1 from random search on test data = 0.92308.\n"
     ]
    }
   ],
   "source": [
    "random_hyp = evaluate(random_results, name = 'random search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
